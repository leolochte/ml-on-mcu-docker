{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fluF3_oOgkWF"
      },
      "source": [
        "# <color style=\"color:red\">**!! Disclaimer !!**</color>\n",
        "\n",
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AJs7HHFmg1M9"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <color style=\"color:green\">**Contributions and License**</color>\n",
        "This notebook is licensed under the [Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0).\n",
        "We thank the following contributors for their contributions to this notebook:\n",
        "* Viviane Potocnik <vivianep@iis.ee.ethz.ch> (ETH Zurich)\n",
        "* TensorFlow team [https://www.tensorflow.org/](https://www.tensorflow.org/)\n",
        "* Leonard Lochte-Holtgreven <lleonard@ethz.ch> (ETH Zurich)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYysdyb-CaWM"
      },
      "source": [
        "# Simple audio recognition: Recognizing keywords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPfDNFlb66XF"
      },
      "source": [
        "This tutorial demonstrates how to preprocess audio files in the WAV format and build and train a basic [automatic speech recognition](https://en.wikipedia.org/wiki/Speech_recognition) (ASR) model for recognizing ten different words. You will use a portion of the [Speech Commands dataset](https://www.tensorflow.org/datasets/catalog/speech_commands) ([Warden, 2018](https://arxiv.org/abs/1804.03209)), which contains short (one-second or less) audio clips of commands, such as \"down\", \"go\", \"left\", \"no\", \"right\", \"stop\", \"up\" and \"yes\".\n",
        "\n",
        "Real-world speech and audio recognition [systems](https://ai.googleblog.com/search/label/Speech%20Recognition) are complex. But, like in the previous weeks, this tutorial should give you a basic understanding of the techniques involved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go9C3uLL8Izc"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Import necessary modules and dependencies. You'll be using `tf.keras.utils.audio_dataset_from_directory` (introduced in TensorFlow 2.10), which helps generate audio classification datasets from directories of `.wav` files. You'll also need [seaborn](https://seaborn.pydata.org) for visualization in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzLKpmZICaWN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from IPython import display\n",
        "\n",
        "# Set the seed value for experiment reproducibility.\n",
        "seed = 42\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR0EdgrLCaWR"
      },
      "source": [
        "## Import the mini Speech Commands dataset\n",
        "\n",
        "To save time with data loading, you will be working with a smaller version of the Speech Commands dataset. The [original dataset](https://www.tensorflow.org/datasets/catalog/speech_commands) consists of over 105,000 audio files in the [WAV (Waveform) audio file format](https://www.aelius.com/njh/wavemetatools/doc/riffmci.pdf) of people saying 35 different words. This data was collected by Google and released under a CC BY license.\n",
        "\n",
        "Download and extract the `mini_speech_commands.zip` file containing the smaller Speech Commands datasets with `tf.keras.utils.get_file`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-rayb7-3Y0I"
      },
      "outputs": [],
      "source": [
        "DATASET_PATH = 'data/mini_speech_commands_extracted/mini_speech_commands'\n",
        "\n",
        "data_dir = pathlib.Path(DATASET_PATH)\n",
        "if not data_dir.exists():\n",
        "  tf.keras.utils.get_file(\n",
        "      'mini_speech_commands.zip',\n",
        "      origin=\"http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip\",\n",
        "      extract=True,\n",
        "      cache_dir='.', cache_subdir='data')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgvFq3uYiS5G"
      },
      "source": [
        "The dataset's audio clips are stored in eight folders corresponding to each speech command: `no`, `yes`, `down`, `go`, `left`, `up`, `right`, and `stop`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70IBxSKxA1N9"
      },
      "outputs": [],
      "source": [
        "commands = np.array(tf.io.gfile.listdir(str(data_dir)))\n",
        "commands = commands[(commands != 'README.md') & (commands != '.DS_Store')]\n",
        "print('Commands:', commands)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ7GJjDvHqtt"
      },
      "source": [
        "Divided into directories this way, you can easily load the data using `keras.utils.audio_dataset_from_directory`. \n",
        "\n",
        "The audio clips are 1 second or less at 16kHz. The `output_sequence_length=16000` pads the short ones to exactly 1 second (and would trim longer ones) so that they can be easily batched."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFM4c3aMC8Qv"
      },
      "outputs": [],
      "source": [
        "train_ds, val_ds = tf.keras.utils.audio_dataset_from_directory(\n",
        "    directory=data_dir,\n",
        "    batch_size=64,\n",
        "    validation_split=0.2,\n",
        "    seed=0,\n",
        "    output_sequence_length=16000,\n",
        "    subset='both')\n",
        "\n",
        "label_names = np.array(train_ds.class_names)\n",
        "print()\n",
        "print(\"label names:\", label_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppG9Dgq2Ex8R"
      },
      "source": [
        "By default, `keras.utils.audio_dataset_from_directory` includes a `num_channels` dimension. This dataset only contains single channel audio, so use the `tf.squeeze` function to drop the extra axis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xl-tnniUIBlM"
      },
      "outputs": [],
      "source": [
        "def squeeze(audio, labels):\n",
        "  audio = tf.squeeze(audio, axis=-1)\n",
        "  return audio, labels\n",
        "\n",
        "train_ds = train_ds.map(squeeze, tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.map(squeeze, tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtsCSWZN5ILv"
      },
      "source": [
        "The `utils.audio_dataset_from_directory` function only returns up to two splits. It's a good idea to keep a test set separate from your validation set.\n",
        "Ideally you'd keep it in a separate directory, but in this case you can use `Dataset.shard` to split the validation set into two halves. Note that iterating over **any** shard will load **all** the data, and only keep its fraction. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5UEGsqM5Gss"
      },
      "outputs": [],
      "source": [
        "test_ds = val_ds.shard(num_shards=2, index=0)\n",
        "val_ds = val_ds.shard(num_shards=2, index=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's print the shape of one item in the `train_ds`! You should see that it consists of a batch with 64 audio files, each containing a sequence of 16000 samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIeoJcwJH5h9"
      },
      "outputs": [],
      "source": [
        "for example_audio, example_labels in train_ds.take(1):  \n",
        "  print(example_audio.shape)\n",
        "  print(example_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voxGEwvuh2L7"
      },
      "source": [
        "Let's plot a few audio waveforms:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yuX6Nqzf6wT"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16, 10))\n",
        "rows = 3\n",
        "cols = 3\n",
        "n = rows * cols\n",
        "for i in range(n):\n",
        "  plt.subplot(rows, cols, i+1)\n",
        "  audio_signal = example_audio[i]\n",
        "  plt.plot(audio_signal)\n",
        "  plt.title(label_names[example_labels[i]])\n",
        "  plt.yticks(np.arange(-1.2, 1.2, 0.2))\n",
        "  plt.ylim([-1.1, 1.1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convert Waveforms to MFCCs\n",
        "\n",
        "The waveforms in the dataset are represented in the time domain. Next, you'll transform the waveforms from the time-domain signals into feature representations by computing the Mel-frequency cepstral coefficients (MFCCs). MFCCs capture the spectral characteristics of the audio signal and are commonly used as input for machine learning models.\n",
        "\n",
        "A Fourier transform (`tf.signal.fft`) converts a signal to its component frequencies, losing all time information. In comparison, the short-time Fourier transform (`tf.signal.stft`) splits the signal into windows of time and runs a Fourier transform on each window, preserving some time information. This results in a 2D tensor that can be further processed to compute MFCCs.\n",
        "\n",
        "### Create a Utility Function for Converting Waveforms to MFCCs:\n",
        "\n",
        "- **Length Consistency**: Ensure that the waveforms are of consistent length. This can be done by zero-padding audio clips that are shorter than one second (using `tf.zeros`). This ensures that all input samples have the same duration, which is crucial for training a neural network.\n",
        "\n",
        "- **STFT Parameters**: When calling `tf.signal.stft`, choose the `frame_length` and `frame_step` parameters to effectively capture the desired time-frequency characteristics. A common choice is to set these values so that the resulting representation provides a good balance between time and frequency resolution.\n",
        "\n",
        "- **Magnitude Calculation**: After computing the STFT, derive the magnitude of the complex numbers using `tf.abs`. The magnitude captures the intensity of each frequency component without the phase information, which is not necessary for MFCC extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_mfcc(waveform, sample_rate=16000, num_mel_bins=20):\n",
        "    # Convert the waveform to a spectrogram via a STFT.\n",
        "    stft = tf.signal.stft(waveform, frame_length=1024, frame_step=512)\n",
        "    \n",
        "    # Obtain the magnitude of the STFT.\n",
        "    magnitude_spectrogram = tf.abs(stft)\n",
        "    \n",
        "    # Convert to a log mel spectrogram.\n",
        "    mel_filterbank = tf.signal.linear_to_mel_weight_matrix(\n",
        "        num_mel_bins, magnitude_spectrogram.shape[-1], sample_rate)\n",
        "    \n",
        "    mel_spectrogram = tf.matmul(magnitude_spectrogram, mel_filterbank)\n",
        "    \n",
        "    # Convert to log scale.\n",
        "    log_mel_spectrogram = tf.math.log(mel_spectrogram + 1e-6)\n",
        "    \n",
        "    # Compute MFCCs from the log mel spectrogram.\n",
        "    mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrogram)\n",
        "    \n",
        "    # Add a channels dimension for compatibility with CNNs.\n",
        "    mfccs = mfccs[..., tf.newaxis]\n",
        "    \n",
        "    return mfccs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rdPiPYJphs2"
      },
      "source": [
        "Next, start exploring the data. Print the shapes of one example's tensorized waveform and the corresponding spectrogram, and play the original audio:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Mu6Y7Yz3C-V"
      },
      "outputs": [],
      "source": [
        "for i in range(3):\n",
        "  label = label_names[example_labels[i]]\n",
        "  waveform = example_audio[i]\n",
        "  #spectrogram = get_spectrogram(waveform)\n",
        "  mfcc = get_mfcc(waveform)\n",
        "\n",
        "  print('Label:', label)\n",
        "  print('Waveform shape:', waveform.shape)\n",
        "  print('MFCC shape:', mfcc.shape)\n",
        "  print('Audio playback')\n",
        "  display.display(display.Audio(waveform, rate=16000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnSuqyxJ1isF"
      },
      "source": [
        "Now, define a function for displaying a spectrogram:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_mfcc(mfcc, ax):\n",
        "    if len(mfcc.shape) > 2:\n",
        "        assert len(mfcc.shape) == 3\n",
        "        mfcc = np.squeeze(mfcc, axis=-1)  # Remove the channel dimension\n",
        "    \n",
        "    # Transpose the MFCCs for plotting (time on x-axis, MFCC coefficients on y-axis).\n",
        "    mfcc = mfcc.T\n",
        "    \n",
        "    # Plotting the MFCCs as discrete pixels\n",
        "    ax.imshow(mfcc, aspect='auto', origin='lower', interpolation='nearest', cmap='viridis')  # Use 'nearest' for discrete effect\n",
        "    ax.set_ylabel('MFCC Coefficients')\n",
        "    ax.set_xlabel('Time Frames')\n",
        "    ax.set_title('MFCCs')\n",
        "    plt.colorbar(ax.imshow(mfcc, aspect='auto', origin='lower', interpolation='nearest', cmap='viridis'), ax=ax)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baa5c91e8603"
      },
      "source": [
        "Plot the example's waveform over time and the corresponding spectrogram (frequencies over time):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming `waveform` is your audio signal and `mfcc` is your computed MFCCs\n",
        "fig, axes = plt.subplots(2, figsize=(12, 8))\n",
        "\n",
        "# Plot the waveform\n",
        "timescale = np.arange(waveform.shape[0])\n",
        "axes[0].plot(timescale, waveform.numpy())\n",
        "axes[0].set_title('Waveform')\n",
        "axes[0].set_xlim([0, 16000])  # Adjust according to your sample rate\n",
        "\n",
        "# Plot the MFCCs using the modified plot_mfcc function\n",
        "plot_mfcc(mfcc, axes[1])  # Use the updated plot function for MFCCs\n",
        "axes[1].set_title('MFCCs')\n",
        "\n",
        "plt.suptitle(label.title())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyYXjW07jCHA"
      },
      "source": [
        "Now, create MFCC datasets from the audio datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_mfcc_ds(ds):\n",
        "  return ds.map(\n",
        "      map_func=lambda audio,label: (get_mfcc(audio), label),\n",
        "      num_parallel_calls=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_mfcc_ds = make_mfcc_ds(train_ds)\n",
        "val_mfcc_ds = make_mfcc_ds(val_ds)\n",
        "test_mfcc_ds = make_mfcc_ds(test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gQpAAgMnyDi"
      },
      "source": [
        "Examine the MFCCs for different examples of the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for example_mfcc, example_mfcc_labels in train_mfcc_ds.take(1):\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUbHfTuon4iF"
      },
      "outputs": [],
      "source": [
        "rows = 3\n",
        "cols = 3\n",
        "n = rows*cols\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(16, 9))\n",
        "\n",
        "for i in range(n):\n",
        "    r = i // cols\n",
        "    c = i % cols\n",
        "    ax = axes[r][c]\n",
        "    plot_mfcc(example_mfcc[i].numpy(), ax)\n",
        "    ax.set_title(label_names[example_mfcc_labels[i].numpy()])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5KdY8IF8rkt"
      },
      "source": [
        "## Build and train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GS1uIh6F_TN9"
      },
      "source": [
        "Add `Dataset.cache` and `Dataset.prefetch` operations to reduce read latency while training the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_mfcc_ds = train_mfcc_ds.cache().shuffle(10000).prefetch(tf.data.AUTOTUNE)\n",
        "val_mfcc_ds = val_mfcc_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
        "test_mfcc_ds = test_mfcc_ds.cache().prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwHkKCQQb5oW"
      },
      "source": [
        "For the model, you'll use a simple convolutional neural network (CNN), since you have transformed the audio files into MFCC images.\n",
        "\n",
        "Your `tf.keras.Sequential` model will use the following Keras preprocessing layers:\n",
        "\n",
        "- `tf.keras.layers.Resizing`: to downsample the input to enable the model to train faster.\n",
        "- `tf.keras.layers.Normalization`: to normalize each pixel in the image based on its mean and standard deviation.\n",
        "\n",
        "For the `Normalization` layer, its `adapt` method would first need to be called on the training data in order to compute aggregate statistics (that is, the mean and the standard deviation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALYz7PFCHblP"
      },
      "outputs": [],
      "source": [
        "input_shape = example_mfcc.shape[1:]\n",
        "print('Input shape:', input_shape)\n",
        "num_labels = len(label_names)\n",
        "\n",
        "# Instantiate the `tf.keras.layers.Normalization` layer.\n",
        "norm_layer = layers.Normalization()\n",
        "# Fit the state of the layer to the spectrograms\n",
        "# with `Normalization.adapt`.\n",
        "norm_layer.adapt(data=train_mfcc_ds.map(map_func=lambda mfcc, label: mfcc))\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=input_shape),\n",
        "    norm_layer,\n",
        "    layers.Conv2D(32, 3, activation='relu'),\n",
        "    layers.Conv2D(64, 3, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Dropout(0.25),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(num_labels),\n",
        "])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de52e5afa2f3"
      },
      "source": [
        "Configure the Keras model with the Adam optimizer and the cross-entropy loss:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFjj7-EmsTD-"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f42b9e3a4705"
      },
      "source": [
        "Train the model over 10 epochs for demonstration purposes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttioPJVMcGtq"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10\n",
        "history = model.fit(\n",
        "    train_mfcc_ds,\n",
        "    validation_data=val_mfcc_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjpCDeQ4mUfS"
      },
      "source": [
        "Let's plot the training and validation loss curves to check how your model has improved during training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzhipg3Gu2AY"
      },
      "outputs": [],
      "source": [
        "metrics = history.history\n",
        "plt.figure(figsize=(16,6))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n",
        "plt.legend(['loss', 'val_loss'])\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss [CrossEntropy]')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.epoch, 100*np.array(metrics['accuracy']), 100*np.array(metrics['val_accuracy']))\n",
        "plt.legend(['accuracy', 'val_accuracy'])\n",
        "plt.ylim([0, 100])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy [%]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZTt3kO3mfm4"
      },
      "source": [
        "## Evaluate the model performance\n",
        "\n",
        "Run the model on the test set and check the model's performance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FapuRT_SsWGQ"
      },
      "outputs": [],
      "source": [
        "model.evaluate(test_mfcc_ds, return_dict=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "en9Znt1NOabH"
      },
      "source": [
        "### Display a confusion matrix\n",
        "\n",
        "Use a [confusion matrix](https://developers.google.com/machine-learning/glossary#confusion-matrix) to check how well the model did classifying each of the commands in the test set:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Y6vmWWQuuT1"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(test_mfcc_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6F0il82u7lW"
      },
      "outputs": [],
      "source": [
        "y_pred = tf.argmax(y_pred, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHSNoBYLvX81"
      },
      "outputs": [],
      "source": [
        "y_true = tf.concat(list(test_mfcc_ds.map(lambda s,lab: lab)), axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvoSAOiXU3lL"
      },
      "outputs": [],
      "source": [
        "confusion_mtx = tf.math.confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(confusion_mtx,\n",
        "            xticklabels=label_names,\n",
        "            yticklabels=label_names,\n",
        "            annot=True,\n",
        "            fmt='g',\n",
        "            cmap='Blues')\n",
        "plt.xlabel('Prediction')\n",
        "plt.ylabel('Label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQGi_mzPcLvl"
      },
      "source": [
        "## Run inference on an audio file\n",
        "\n",
        "Finally, verify the model's prediction output using an input audio file of someone saying \"no\". How well does your model perform?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRxauKMdhofU"
      },
      "outputs": [],
      "source": [
        "x = data_dir/'left/0b09edd3_nohash_0.wav'\n",
        "x = tf.io.read_file(str(x))\n",
        "x, sample_rate = tf.audio.decode_wav(x, desired_channels=1, desired_samples=16000,)\n",
        "x = tf.squeeze(x, axis=-1)\n",
        "waveform = x\n",
        "x = get_mfcc(x)\n",
        "x = x[tf.newaxis,...]\n",
        "\n",
        "prediction = model(x)\n",
        "x_labels = ['down', 'go', 'left', 'no', 'right', 'stop', 'up', 'yes']\n",
        "plt.bar(x_labels, tf.nn.softmax(prediction[0]))\n",
        "plt.title('Left')\n",
        "plt.show()\n",
        "\n",
        "display.display(display.Audio(waveform, rate=16000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgWICqdqQNaQ"
      },
      "source": [
        "As the output suggests, your model should have recognized the audio command as \"left\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TFLite Conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open('model.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's load the `model.tflite` file to check if everything is working properly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the TFLite model and allocate tensors\n",
        "interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output tensors\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a new validation set with batch size 1 so we can use it easily on the TFLite model\n",
        "val_ds_batch1 = tf.keras.utils.audio_dataset_from_directory(\n",
        "    directory=data_dir,\n",
        "    batch_size=1,\n",
        "    validation_split=0.2,\n",
        "    seed=0,\n",
        "    output_sequence_length=16000,\n",
        "    subset='validation')\n",
        "\n",
        "val_ds_batch1 = val_ds_batch1.map(squeeze, tf.data.AUTOTUNE)\n",
        "val_mfcc_ds_batch1 = make_mfcc_ds(val_ds_batch1)\n",
        "\n",
        "# Save one for later\n",
        "for example_audio, example_labels in val_mfcc_ds_batch1.take(1):  \n",
        "    print(example_audio.shape)\n",
        "    print(example_labels.shape)\n",
        "    np.save('sample.npy', example_audio.numpy())\n",
        "    np.save('label.npy', example_labels.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_accuracy(dataset):\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for features, labels in dataset:\n",
        "        interpreter.set_tensor(input_details[0]['index'], features)\n",
        "        interpreter.invoke()\n",
        "\n",
        "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "        predictions = np.argmax(output_data, axis=1)\n",
        "\n",
        "        correct_predictions += np.sum(predictions == labels.numpy())\n",
        "        total_samples += labels.shape[0]\n",
        "\n",
        "    accuracy = correct_predictions / total_samples\n",
        "    return accuracy\n",
        "\n",
        "accuracy = compute_accuracy(val_mfcc_ds_batch1)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can check the size of our TFLite file in Mb:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "model_size_bytes = os.path.getsize('model.tflite')\n",
        "model_size_mb = model_size_bytes / (1024 * 1024)\n",
        "\n",
        "print(f\"TFLite file size: {model_size_mb:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3jF933m9z1J"
      },
      "source": [
        "## Further Resources\n",
        "\n",
        "This tutorial demonstrated how to carry out simple audio classification/automatic speech recognition using a convolutional neural network with TensorFlow and Python. To learn more, consider the following resources:\n",
        "\n",
        "- The [Sound classification with YAMNet](https://www.tensorflow.org/hub/tutorials/yamnet) tutorial shows how to use transfer learning for audio classification.\n",
        "- The notebooks from [Kaggle's TensorFlow speech recognition challenge](https://www.kaggle.com/c/tensorflow-speech-recognition-challenge/overview).\n",
        "- The \n",
        "[TensorFlow.js - Audio recognition using transfer learning codelab](https://codelabs.developers.google.com/codelabs/tensorflowjs-audio-codelab/index.html#0) teaches how to build your own interactive web app for audio classification.\n",
        "- [A tutorial on deep learning for music information retrieval](https://arxiv.org/abs/1709.04396) (Choi et al., 2017) on arXiv.\n",
        "- TensorFlow also has additional support for [audio data preparation and augmentation](https://www.tensorflow.org/io/tutorials/audio) to help with your own audio-based projects.\n",
        "- Consider using the [librosa](https://librosa.org/) library for music and audio analysis."
      ]
    }
  ],
  "metadata": {
    "accelerator": "CPU",
    "colab": {
      "collapsed_sections": [],
      "name": "simple_audio.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
